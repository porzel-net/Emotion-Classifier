\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{float}

\title{Model Architecture and Design Philosophy}
\author{Necati Deniz Baykuş}
\date{December 2025}

\begin{document}

\maketitle

\section{Introduction}
The success of the emotion recognition model relies heavily on an architecture that can extract discriminative features from a limited $64 \times 64$ pixel budget while maintaining low inference latency for real-time deployment.

\section{Algorithm Survey and Model Selection}
We evaluated three distinct architectural paradigms to identify the optimal balance between depth, parameter efficiency, and generalization capability:

Baseline CNN (LeNet-style): A shallow architecture utilizing 2-3 convolutional layers. While computationally inexpensive, it lacks the representational power to capture the subtle micro-expressions of the human face  required for 6-class classification. \cite{patternrecognition}

VGG-16 / AlexNet: These models use large filter banks and deep stacks of convolutions. However, their high parameter count (up to 138M) poses a significant risk of overfitting on specialized datasets like FER-2013 and increases the computational load for the live demo phase.

Residual Networks (ResNet-18): This model introduces skip connections (shortcuts) that bypass one or more layers. Instead of learning an entirely new mapping, the network learns the residual (the difference) between the input and output. This "identity mapping" ensures that critical facial features are preserved across deep layers, leading to more stable training and higher accuracy for 64x64 images.


Decision: We selected ResNet-18 (\ref{fig:diagram}) as our core architecture due to its superior accuracy-to-parameter ratio and its native compatibility with Explainable AI (XAI) methods.
\cite{resnetfacial}


\subsection{Proposed Architecture: Modified ResNet-18}
The standard ResNet-18 is designed for $224 \times 224$ ImageNet samples. To accommodate our $64 \times 64$ input resolution, we propose several architectural modifications:

1.	Initial Convolution Layer: The standard $7 \times 7$ kernel (stride 2) is replaced with a $3 \times 3$ kernel (stride 1). This ensures that critical spatial information is not discarded in the very first layer given the low-resolution input.


2.	Feature Extraction Blocks: The model consists of four stages of residual blocks. Each block employs Batch Normalization and ReLU activation to stabilize the internal covariate shift, which is crucial when dealing with the multi-source data distributions mentioned in the Data Engineering strategy.

3.	Global Average Pooling (GAP): Instead of using a traditional Flatten layer followed by high-dimensional Fully Connected (FC) layers, we utilize GAP. This reduces the total parameter count and makes the model robust against spatial translations of the face.

4.	Softmax Classifier: The final layer is a 6-node FC layer with Softmax activation, outputting the probability distribution across the target emotions: Happiness, Surprise, Sadness, Anger, Disgust, and Fear.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.6\textwidth]{resnet18.jpg}
  \caption{Structure of the standard ResNet18 network, color and shape networks. Every layer (represented in a different color) sums (+) its activation at every residual block.}
  \label{fig:diagram}
\end{figure}



\subsection{Hyperparameter Landscape and Search Space}
To optimize the training process, we have defined the following hyperparameter search space:

•	Optimizer: Adam ($\beta_1=0.9, \beta_2=0.999$) for its adaptive learning rate properties, with a fallback to SGD with Momentum if generalization plateaus.

•	Learning Rate Strategy: A base rate of $\eta = 1e-3$ with a ReduceLROnPlateau scheduler to refine weights as the loss converges.

•	Regularization: To prevent overfitting, we implement a Dropout rate of 0.3 in the final layers and an $L_2$ Weight Decay of $1e-4$.

•	Loss Function: We utilize Weighted Cross-Entropy Loss to compensate for the class imbalance inherent in the FER datasets, ensuring the model does not ignore minority classes like Disgust.
\cite{resnethyper}

\section{Design for Interpretability}

A primary requirement of this project is to provide visual explanations. Our architecture is designed to support Grad-CAM (Gradient-weighted Class Activation Mapping). By maintaining the spatial integrity of the feature maps until the GAP layer, we can back-propagate the gradients of the winning emotion class to the final convolutional layer. This will generate Saliency Maps that highlight which facial regions (e.g., eyes for Surprise, mouth for Happiness) the model prioritized for its prediction.

\bibliographystyle{unsrt}
\bibliography{references}


\end{document}
