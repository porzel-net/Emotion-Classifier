\section{Data strategy and preprocessing}
\label{sec:data}

\noindent\textit{(Research lead: Julius Porzel)}

Training a six-class emotion classifier on fixed $64\times64$ inputs raises two intertwined constraints: the images are too small to resolve fine-grained micro-expressions, and the taxonomy (happy, surprise, sadness, anger, disgust, fear) amplifies the already strong class imbalance once the neutral category is removed. The research therefore sets these constraints as first principles: geometric nuisance such as pose, scale, and skew must be eliminated before the model sees any pixels, the scarce discriminative information must not be wasted on uninformative details, and every optimisation step must try to prevent the model from collapsing onto the majority classes \cite{mazen2021balanced, fernando2021dwb, yalcin2024novel}.

To reach competitive generalisation, multiple datasets are merged instead of relying on a single source. FER-2013 is retained for its structured, grayscale $48\times48$ frames but requires a careful upscaling step to match the $64\times64$ requirement. AffectNet-8 contributes hundreds of thousands of $96\times96$ in-the-wild color images with landmark annotations that are helpful for standardizing the face geometry, while RAF-DB adds further variance from real-world captures \cite{fer2013_kaggle2025, affectnet8_kaggle2025, rafdb_kaggle2025}. Joining these datasets also creates secondary challenges: the distribution of samples across emotions becomes even more uneven (AffectNet supplies the bulk of the data) and color spaces or illumination statistics differ from FER-2013. Rather than attempting a naive concatenation, the pipeline orchestrates the sources so that the largest dataset is introduced last, allowing the model to first learn from data that closely resemble the FER-2013 target before being exposed to increasingly harder domains.

Every image is processed through a strict three-stage pipeline. RetinaFace locates the face bounding box and five landmarks (eyes, nose, mouth corners), also providing a dependable foundation for later demo tasks that overlay saliency on the live video \cite{deng2020retinaface}. A similarity transform then stabilizes the face so that key points occupy canonical pixel locations, reducing the need for the subsequent network to be invariant to pose. For FER-2013 inputs, two specialised super-resolution options are considered: eigenface-domain SR prioritizes discriminative coefficients within a reduced dimensional face space, and zero-shot SR trains a tiny CNN on the single input image at test time to adapt to its specific noise/artefacts. Both minimize blurring from naïve interpolation while retaining the richest features possible in the $64\times64$ budget \cite{gunturk2003eigenface, shocher2018zssr}.

Combining multiple domains also triggers “negative transfer,” where highly divergent samples might degrade the core FER-2013 accuracy. The adopted solution is a progressive multi-source domain adaptation flow: start training with the subset of source samples most similar to the target distribution and progressively introduce harder domains only once the new data surpasses a relevance threshold. A density-aware memory keeps track of previously seen, beneficial samples so the network does not catastrophically forget earlier knowledge, ensuring that the training path remains smooth as new domains are added. The final stage, BORT$^2$ (Bi-level Optimization based Robust Target Training), fine-tunes the model on pseudo-labeled target samples, explicitly modeling label uncertainty via an entropy maximization regularizer so that noisy self-labels do not drown out the true expressions \cite{chen2019progressive, deng2022bort2}.

This multi-faceted strategy demands time and engineering effort, which is why the current implementation plan starts with a single dataset to validate the preprocessing and training loops. Once the baseline is stable, the progressive domain introduction and BORT$^2$ steps will be activated to assess whether the increased data variance actually translates into robustness gains for the downstream live classifier.
